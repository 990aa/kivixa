[package]
name = "kivixa_native"
version = "1.0.0"
edition = "2021"
description = "Native Rust engine for Kivixa AI features"
authors = ["990aa"]


[lints.rust]
unexpected_cfgs = { level = "warn", check-cfg = ['cfg(frb_expand)'] }

[lib]
name = "kivixa_native"
crate-type = ["cdylib", "staticlib"]

[dependencies]
# Flutter Rust Bridge for Dart interop
flutter_rust_bridge = "=2.11.1"

# Error handling
anyhow = "1.0"

# Llama.cpp bindings for Phi-4 inference
# Default: CPU mode (no GPU features) - safe for all platforms
llama-cpp-2 = { version = "0.1.79", default-features = false }

# Force-directed graph simulation for knowledge graph
fdg-sim = "0.9"

# Machine Learning for K-Means clustering
linfa = "0.7"
linfa-clustering = "0.7"
ndarray = "0.15"

# Serialization for graph data
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Logging
log = "0.4"
env_logger = "0.11"

# Thread-safe primitives
parking_lot = "0.12"

# Lazy statics
once_cell = "1.19"

# Async runtime (for streaming responses)
tokio = { version = "1", features = ["rt-multi-thread", "sync"] }

# ============================================================================
# Platform-Specific GPU Backends (Hybrid Performance Strategy)
# ============================================================================

# Windows/Linux: CPU mode only (AVX2 auto-detected, no SDK required)
# llama.cpp is highly optimized for CPUs - 10-15 tokens/sec on modern CPUs
[target.'cfg(any(target_os = "windows", target_os = "linux"))'.dependencies]
llama-cpp-sys-2 = { version = "0.1.79", default-features = false }

# Android: Vulkan for GPU acceleration (NDK includes Vulkan headers)
[target.'cfg(target_os = "android")'.dependencies]
llama-cpp-sys-2 = { version = "0.1.130" }

# macOS/iOS: Metal for Apple Silicon GPU acceleration
[target.'cfg(any(target_os = "macos", target_os = "ios"))'.dependencies]
llama-cpp-sys-2 = { version = "0.1.79", features = ["metal"] }

[build-dependencies]
cc = "1.0"

[profile.release]
lto = true
codegen-units = 1
opt-level = 3
strip = true

[patch.crates-io]
llama-cpp-sys-2 = { path = "vendor/llama-cpp-sys-2" }
